{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pdftojsonl import process_pdfs\n",
    "\n",
    "process_pdfs('c:\\\\tools\\\\code\\\\ResumeAnalytics\\\\Candidate Resumes', 'candidate.jsonl')\n",
    "process_pdfs('c:\\\\tools\\\\code\\\\ResumeAnalytics\\\\Target Resumes', 'target.jsonl')\n",
    "\n",
    "import json\n",
    "with open('candidate.jsonl', 'r') as cf, open('target.jsonl', 'r') as tf:\n",
    "  candidate = [json.loads(line) for line in cf]\n",
    "  target = [json.loads(line) for line in tf]\n",
    "    \n",
    "mashed_target = ''\n",
    "doc_set = []\n",
    "for t in target:\n",
    "  mashed_target += t['content']\n",
    "  doc_set.append(t['content'])\n",
    "target = [{'name':'target', 'content': mashed_target}]\n",
    "\n",
    "can_set = []\n",
    "for c in candidate:\n",
    "  can_set.append(c['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.001*develop + 0.001*comput + 0.001*research + 0.001*use'), (1, '0.001*experi + 0.001*9 + 0.001*stanford + 0.001*analysi'), (2, '0.001*develop + 0.001*univers + 0.001*work + 0.001*experi'), (3, '0.001*develop + 0.001*project + 0.001*research + 0.001*experi'), (4, '0.001*project + 0.001*librari + 0.001*develop + 0.001*work'), (5, '0.023*project + 0.013*develop + 0.012*app + 0.012*librari'), (6, '0.001*project + 0.001*develop + 0.001*app + 0.001*work'), (7, '0.001*project + 0.001*app + 0.001*work + 0.001*develop'), (8, '0.001*develop + 0.001*comput + 0.001*b + 0.001*mathemat'), (9, '0.016*experi + 0.016*9 + 0.016*stanford + 0.013*va'), (10, '0.001*research + 0.001*comput + 0.001*develop + 0.001*applic'), (11, '0.001*stanford + 0.001*va + 0.001*06 + 0.001*work'), (12, '0.001*project + 0.001*develop + 0.001*app + 0.001*end'), (13, '0.017*univers + 0.017*st + 0.017*kenyon + 0.017*loui'), (14, '0.020*research + 0.016*develop + 0.013*web + 0.013*comput'), (15, '0.001*develop + 0.001*project + 0.001*research + 0.001*web'), (16, '0.001*develop + 0.001*comput + 0.001*softwar + 0.001*univers'), (17, '0.001*experi + 0.001*develop + 0.001*research + 0.001*engin'), (18, '0.001*project + 0.001*librari + 0.001*develop + 0.001*work'), (19, '0.001*softwar + 0.001*develop + 0.001*univers + 0.001*comput')]\n"
     ]
    }
   ],
   "source": [
    "print (ldamodel.print_topics(num_topics = 20, num_words = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 0.99666666666295112)]\n",
      "[(14, 0.99654545454111176)]\n",
      "[(5, 0.99872654155047613)]\n",
      "[(13, 0.99543269229798748)]\n",
      ">>>>Candidates:<<<<\n",
      "\n",
      "\n",
      "Aberra Aimen 7494924 Resume.txt: 0.996172248804\n",
      "[(5, 0.37504208109239018), (9, 0.17951671068742173), (13, 0.20064036259802667), (14, 0.24097309442598883)]\n",
      "Agnihotri Vasant 4883545 Resume.txt: 0.998464491363\n",
      "[(5, 0.50471367471945328), (9, 0.13683959118554778), (13, 0.052641003600650632), (14, 0.30427022185711261)]\n",
      "Armstrong Mike 6086999 Resume.txt: 0.995789473684\n",
      "[(5, 0.5059493437786472), (9, 0.22216904009075572), (13, 0.07893224467916117), (14, 0.18873884513564596)]\n",
      "Bederu Mariam 6728650 Resume.txt: 0.9961352657\n",
      "[(5, 0.45228752792708787), (9, 0.14570385962745505), (13, 0.085520162511582704), (14, 0.3126237156343572)]\n",
      "Chen John 5261366 Resume.txt: 0.99794344473\n",
      "[(5, 0.58230993939984288), (9, 0.21253273694014654), (13, 0.043644951754066727), (14, 0.15945581663602063)]\n",
      "Coleman Brad 621556 Resume.txt: 0.996850393701\n",
      "[(5, 0.48772696184732395), (9, 0.11211643608805226), (13, 0.10854182324563906), (14, 0.28846517251977177)]\n",
      "Duda Chris 6196252 Resume.txt: 0.998862019915\n",
      "[(5, 0.46664930399723431), (9, 0.21686571829304274), (13, 0.070398105786072765), (14, 0.24494889183830193)]\n",
      "Hamilton Tiffany 5538608 Resume.txt: 0.994838709677\n",
      "[(5, 0.30667810151241448), (9, 0.25581606785381006), (13, 0.19456071955103585), (14, 0.23778382076015872)]\n",
      "Hayes Andrew 6861994 Resume.txt: 0.995918367347\n",
      "[(5, 0.2707273034494394), (9, 0.15153425021816802), (13, 0.13083890984745214), (14, 0.44281790383187902)]\n",
      "Islam Jannatul 7421017 Resume.txt: 0.998573975045\n",
      "[(5, 0.6794235543362368), (9, 0.16425855468732178), (13, 0.0247896201176484), (14, 0.13010224590335662)]\n",
      "Jones Amy 7573738 Resume.txt: 0.998380566802\n",
      "[(5, 0.46262550230653043), (9, 0.1730303225807068), (13, 0.073964656256768069), (14, 0.28876008565761385)]\n",
      "Joshua Scott Resume.txt: 0.99880239521\n",
      "[(5, 0.45863505078329175), (9, 0.17017216080170228), (13, 0.07981793464136025), (14, 0.29017724898322683)]\n",
      "Judge Jacqueline 4989086 Resume.txt: 0.9984375\n",
      "[(5, 0.43300529680482686), (9, 0.12617045626351098), (13, 0.14336740368443432), (14, 0.29589434324722763)]\n",
      "Kaufmann Karl 7578068 Resume.txt: 0.998233995585\n",
      "[(5, 0.45737716394274203), (9, 0.20097225021233781), (13, 0.11228712247013001), (14, 0.22759745895977884)]\n",
      "Moody Andrea 5172377 Resume.txt: 0.998742138365\n",
      "[(5, 0.48943438521575139), (9, 0.20060484122206232), (13, 0.053113390087058759), (14, 0.25558952183990769)]\n",
      "Moon James 7498892 Resume.txt: 0.994736842105\n",
      "[(5, 0.37354029792490229), (9, 0.11143966507464098), (13, 0.19449037955194259), (14, 0.31526649955377717)]\n",
      "Nguyen Ly 7490072 Resume.txt: 0.994482758621\n",
      "[(5, 0.34776973784337262), (9, 0.076519449011841684), (13, 0.21649345054087699), (14, 0.35370012122459821)]\n",
      "Pentela Maruthi 7577891 Resume.txt: 0.999087799316\n",
      "[(5, 0.53939334329985), (9, 0.12620768710727492), (13, 0.055693974663276899), (14, 0.27779279424544812)]\n",
      "Price Michael 7572491 Resume.txt: 0.997530864198\n",
      "[(5, 0.38231368861901122), (9, 0.20488445122480442), (13, 0.12281263830115639), (14, 0.28752008605255869)]\n",
      "Ramirez Phillip 7568846 Resume.txt: 0.994557823129\n",
      "[(5, 0.47060236078194895), (9, 0.19190607605906124), (13, 0.050556088388681676), (14, 0.28149329789955968)]\n",
      "Ritthaworn Isara 7512928 Resume.txt: 0.994838709677\n",
      "[(5, 0.45875777486120523), (9, 0.075240953909769787), (13, 0.18762029446135736), (14, 0.27321968644508676)]\n",
      "Ronald McQueen Resume.txt: 0.998024691358\n",
      "[(5, 0.44605230549618696), (9, 0.10487136746002015), (13, 0.1458202937504593), (14, 0.30128072465135797)]\n",
      "Scott Joshua 6805508 Resume.txt: 0.99880239521\n",
      "[(5, 0.45863458052922079), (9, 0.17017262579769019), (13, 0.079815453757307142), (14, 0.29017973512536305)]\n",
      "Sheils Timothy 7530120 Resume.txt: 0.993162393162\n",
      "[(5, 0.39760629196880604), (9, 0.13065747134518507), (13, 0.26381452735373195), (14, 0.20108410249467029)]\n",
      "Stodd Colin 4376851 Resume.txt: 0.995505617978\n",
      "[(5, 0.35878738053316717), (9, 0.063141123366834334), (13, 0.25213630308979257), (14, 0.32144081098773375)]\n",
      "Viertel_Eli_resume_May2016_gm7.txt: 0.997394136808\n",
      "[(5, 0.34790179156823869), (9, 0.22456107748448548), (13, 0.083930496066326285), (14, 0.34100077168876691)]\n",
      "Warraich Deepak 5960628 Resume.txt: 0.994871794872\n",
      "[(5, 0.28327794730395334), (9, 0.22266036247750534), (13, 0.21816574021493274), (14, 0.27076774487540317)]\n",
      "Yarie Chris 7573680 Resume.txt: 0.99375\n",
      "[(5, 0.40458479860957791), (9, 0.14876826487523695), (13, 0.20515506690030516), (14, 0.23524186961488033)]\n",
      "Zayed Amani 5756346 Resume.txt: 0.994244604317\n",
      "[(5, 0.3539064238836655), (9, 0.19637288325829025), (13, 0.1706796631236199), (14, 0.27328563405097078)]\n",
      "Zen Linda 4555557 Resume.txt: 0.997953964194\n",
      "[(5, 0.45217808139175097), (9, 0.18735089827704815), (13, 0.10932048296200372), (14, 0.24910450156357017)]\n"
     ]
    }
   ],
   "source": [
    "for i in doc_set:\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    print (ldamodel.get_document_topics(dictionary.doc2bow(stemmed_tokens)))\n",
    "    \n",
    "print (\">>>>Candidates:<<<<\\n\\n\")\n",
    "\n",
    "for i in candidate:\n",
    "    raw = i['content'].lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    i['distance'] = 0;\n",
    "    for topic in ldamodel.get_document_topics(dictionary.doc2bow(stemmed_tokens)):\n",
    "        i['distance'] = i['distance'] + topic[1]\n",
    "    print (i['name'] + \": \" + str(i['distance']))\n",
    "    print (ldamodel.get_document_topics(dictionary.doc2bow(stemmed_tokens)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
